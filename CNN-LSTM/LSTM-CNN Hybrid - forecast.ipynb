{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "#os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2021\n",
    "import random\n",
    "import torch\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "# compare with previous study\n",
    "#data = pd.read_csv(\"/Users/xul2/Desktop/CMU/Research/Covid 19/previous.csv\")\n",
    "\n",
    "# validation & forecasting 7 day ahead\n",
    "#data = pd.read_csv(\"/Users/xul2/Desktop/CMU/Research/Covid 19/7-Day Average.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_forecasting.data.examples import get_stallion_data\n",
    "data = pd.read_csv(\"brazil_patch.csv\")\n",
    "data = data.drop(columns=['Unnamed: 0','date','Gathering','Transport'])\n",
    "cols = data.columns.tolist()\n",
    "cols = cols[-1:]+cols[:-1]\n",
    "k3 =data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>face_coverings</th>\n",
       "      <th>Stay_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35994.714</td>\n",
       "      <td>64.35</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35683.286</td>\n",
       "      <td>64.35</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35622.714</td>\n",
       "      <td>64.35</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35280.143</td>\n",
       "      <td>64.35</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35524.714</td>\n",
       "      <td>64.35</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>5046.714</td>\n",
       "      <td>61.57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>5980.286</td>\n",
       "      <td>61.57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>7243.714</td>\n",
       "      <td>61.57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>8173.714</td>\n",
       "      <td>61.57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>8189.143</td>\n",
       "      <td>47.07</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>366 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     new_cases_smoothed  stringency_index  face_coverings  Stay_home\n",
       "0             35994.714             64.35               4          2\n",
       "1             35683.286             64.35               4          2\n",
       "2             35622.714             64.35               4          2\n",
       "3             35280.143             64.35               4          2\n",
       "4             35524.714             64.35               4          2\n",
       "..                  ...               ...             ...        ...\n",
       "361            5046.714             61.57               3          1\n",
       "362            5980.286             61.57               3          1\n",
       "363            7243.714             61.57               3          1\n",
       "364            8173.714             61.57               3          1\n",
       "365            8189.143             47.07               3          0\n",
       "\n",
       "[366 rows x 4 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import read_csv\n",
    "# from matplotlib import pyplot\n",
    "# # load dataset\n",
    "# dataset = data\n",
    "# values = dataset.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose Country\n",
    "# data_interest = data.loc[data[\"Country\"] == \"India\"]\n",
    "# data_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validate\n",
    "# #data_clear = data_interest.drop(columns = [\"Date\",\"Country\",\"Cumulative\",\"Cases\",\"TRUE\",\"7-day\"])\n",
    "\n",
    "# # previous \n",
    "# #data_clear = data_interest.drop(columns = [\"Deaths\", \"Date\",\"Confirmed\",\"Recovered\",\"TRUE\",\"Confirmed\",\"Country\",\"7-day\"])\n",
    "\n",
    "# # forecast\n",
    "# data_clear = data_interest.drop(columns = [\"Date\",\"Country\",\"Cumulative\",\"Cases\",\"TRUE\",\"7-day\"])\n",
    "# data_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)   var1(t)\n",
      "1     0.457580   0.619589        1.0   0.666667  0.453621\n",
      "2     0.453621   0.619589        1.0   0.666667  0.452851\n",
      "3     0.452851   0.619589        1.0   0.666667  0.448496\n",
      "4     0.448496   0.619589        1.0   0.666667  0.451605\n",
      "5     0.451605   0.619589        1.0   0.666667  0.464148\n",
      "..         ...        ...        ...        ...       ...\n",
      "361   0.054498   0.531754        0.0   0.333333  0.064156\n",
      "362   0.064156   0.531754        0.0   0.333333  0.076024\n",
      "363   0.076024   0.531754        0.0   0.333333  0.092085\n",
      "364   0.092085   0.531754        0.0   0.333333  0.103908\n",
      "365   0.103908   0.531754        0.0   0.333333  0.104104\n",
      "\n",
      "[365 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# This has been taken from https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/ \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# load dataset\n",
    "dataset = k3\n",
    "values = dataset.values \n",
    "#values = values[:,0]\n",
    "#values[:,0] = cases\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "#reframed.drop(reframed.columns[[7,8,9,10,11]], axis=1, inplace=True)\n",
    "reframed.drop(reframed.columns[[5,6,7]], axis=1, inplace=True)\n",
    "print(reframed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292, 1, 4) (292,) (1, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "values = reframed.values\n",
    "\n",
    "# evalidate \n",
    "# train = values[:194,:]\n",
    "# forecasting = values[193:194]\n",
    "\n",
    "# forecast 7 day ahead\n",
    "train = values[:292,:]\n",
    "forecasting = values[364:365]\n",
    "\n",
    "# previous\n",
    "# train = values[:130,:]\n",
    "# forecasting = values[129:130]\n",
    "\n",
    "# spli\\t into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "forecast_X = forecasting[:, :-1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "forecast_X = forecast_X.reshape((forecast_X.shape[0], 1, forecast_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, forecast_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# for training\n",
    "class timeseries(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset_train = timeseries(train_X,train_y)\n",
    "#dataloader\n",
    "from torch.utils.data import DataLoader \n",
    "train_loader = DataLoader(dataset_train,shuffle=False,batch_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for forecasting\n",
    "class forecast(Dataset):\n",
    "    def __init__(self,x):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset_forecast = forecast(forecast_X)\n",
    "#dataloader\n",
    "from torch.utils.data import DataLoader \n",
    "forecast_loader = DataLoader(dataset_forecast,shuffle=False,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, prediction, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.prediction = prediction\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        #self.seq_length = seq_length\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=self.hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, int(self.hidden_size/2))\n",
    "        self.fc1 = nn.Linear(int(self.hidden_size/2), self.prediction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.hidden_size)\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        out, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out  = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ForecastNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_ForecastNet,self).__init__()\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3 = 32, 45, 64\n",
    "        self.outputDim = 4\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1,out_channels=self.ch1,kernel_size=2)#5 \n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.ch1,out_channels=self.ch2,kernel_size=2)#4\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.ch2,out_channels=self.ch3,kernel_size=2)#3\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)  \n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)  \n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1,self.ch3) # flatten  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 0.005\n",
    "\n",
    "#input_size = 12\n",
    "input_size = 64\n",
    "hidden_size = 175\n",
    "num_layers = 3\n",
    "\n",
    "prediction = 1\n",
    "\n",
    "lstm = LSTM(prediction, input_size, hidden_size, num_layers)\n",
    "cnn = CNN_ForecastNet()\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "params = list(cnn.parameters()) + list(lstm.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device  = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in lstm.parameters():\n",
    "#     print(param.data[0]) \n",
    "#     a = param.data[0].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_Number:  0\n",
      "Training Loss:  0.561565101146698\n",
      "Epoch_Number:  1\n",
      "Training Loss:  0.4325496554374695\n",
      "Epoch_Number:  2\n",
      "Training Loss:  0.2998915910720825\n",
      "Epoch_Number:  3\n",
      "Training Loss:  0.14356711506843567\n",
      "Epoch_Number:  4\n",
      "Training Loss:  0.0707375779747963\n",
      "Epoch_Number:  5\n",
      "Training Loss:  0.13882172107696533\n",
      "Epoch_Number:  6\n",
      "Training Loss:  0.0684882253408432\n",
      "Epoch_Number:  7\n",
      "Training Loss:  0.05948318913578987\n",
      "Epoch_Number:  8\n",
      "Training Loss:  0.07008606940507889\n",
      "Epoch_Number:  9\n",
      "Training Loss:  0.0763545110821724\n",
      "Epoch_Number:  10\n",
      "Training Loss:  0.07486340403556824\n",
      "Epoch_Number:  11\n",
      "Training Loss:  0.06876491010189056\n",
      "Epoch_Number:  12\n",
      "Training Loss:  0.06256997585296631\n",
      "Epoch_Number:  13\n",
      "Training Loss:  0.059996820986270905\n",
      "Epoch_Number:  14\n",
      "Training Loss:  0.062130071222782135\n",
      "Epoch_Number:  15\n",
      "Training Loss:  0.06626533716917038\n",
      "Epoch_Number:  16\n",
      "Training Loss:  0.0679808035492897\n",
      "Epoch_Number:  17\n",
      "Training Loss:  0.06581995636224747\n",
      "Epoch_Number:  18\n",
      "Training Loss:  0.06214482709765434\n",
      "Epoch_Number:  19\n",
      "Training Loss:  0.059849247336387634\n",
      "Epoch_Number:  20\n",
      "Training Loss:  0.05985364690423012\n",
      "Epoch_Number:  21\n",
      "Training Loss:  0.06120067834854126\n",
      "Epoch_Number:  22\n",
      "Training Loss:  0.062386393547058105\n",
      "Epoch_Number:  23\n",
      "Training Loss:  0.062392089515924454\n",
      "Epoch_Number:  24\n",
      "Training Loss:  0.06106216460466385\n",
      "Epoch_Number:  25\n",
      "Training Loss:  0.05897798761725426\n",
      "Epoch_Number:  26\n",
      "Training Loss:  0.05695439875125885\n",
      "Epoch_Number:  27\n",
      "Training Loss:  0.05539244785904884\n",
      "Epoch_Number:  28\n",
      "Training Loss:  0.05389847978949547\n",
      "Epoch_Number:  29\n",
      "Training Loss:  0.051385488361120224\n",
      "Epoch_Number:  30\n",
      "Training Loss:  0.04643533006310463\n",
      "Epoch_Number:  31\n",
      "Training Loss:  0.03841618448495865\n",
      "Epoch_Number:  32\n",
      "Training Loss:  0.028111280873417854\n",
      "Epoch_Number:  33\n",
      "Training Loss:  0.017848117277026176\n",
      "Epoch_Number:  34\n",
      "Training Loss:  0.012535198591649532\n",
      "Epoch_Number:  35\n",
      "Training Loss:  0.019085679203271866\n",
      "Epoch_Number:  36\n",
      "Training Loss:  0.024474531412124634\n",
      "Epoch_Number:  37\n",
      "Training Loss:  0.01464899256825447\n",
      "Epoch_Number:  38\n",
      "Training Loss:  0.010882489383220673\n",
      "Epoch_Number:  39\n",
      "Training Loss:  0.01382919680327177\n",
      "Epoch_Number:  40\n",
      "Training Loss:  0.015755264088511467\n",
      "Epoch_Number:  41\n",
      "Training Loss:  0.014808591455221176\n",
      "Epoch_Number:  42\n",
      "Training Loss:  0.01280857902020216\n",
      "Epoch_Number:  43\n",
      "Training Loss:  0.011721421033143997\n",
      "Epoch_Number:  44\n",
      "Training Loss:  0.01076376810669899\n",
      "Epoch_Number:  45\n",
      "Training Loss:  0.008047089911997318\n",
      "Epoch_Number:  46\n",
      "Training Loss:  0.007772995159029961\n",
      "Epoch_Number:  47\n",
      "Training Loss:  0.009533335454761982\n",
      "Epoch_Number:  48\n",
      "Training Loss:  0.007133461069315672\n",
      "Epoch_Number:  49\n",
      "Training Loss:  0.006572135724127293\n",
      "Epoch_Number:  50\n",
      "Training Loss:  0.005479660350829363\n",
      "Epoch_Number:  51\n",
      "Training Loss:  0.003313159104436636\n",
      "Epoch_Number:  52\n",
      "Training Loss:  0.0036735774483531713\n",
      "Epoch_Number:  53\n",
      "Training Loss:  0.0034751486964523792\n",
      "Epoch_Number:  54\n",
      "Training Loss:  0.0024870261549949646\n",
      "Epoch_Number:  55\n",
      "Training Loss:  0.0029825542587786913\n",
      "Epoch_Number:  56\n",
      "Training Loss:  0.0023163631558418274\n",
      "Epoch_Number:  57\n",
      "Training Loss:  0.0018351314356550574\n",
      "Epoch_Number:  58\n",
      "Training Loss:  0.002348870038986206\n",
      "Epoch_Number:  59\n",
      "Training Loss:  0.001981934066861868\n",
      "Epoch_Number:  60\n",
      "Training Loss:  0.002473966684192419\n",
      "Epoch_Number:  61\n",
      "Training Loss:  0.0022482427302747965\n",
      "Epoch_Number:  62\n",
      "Training Loss:  0.0018686001421883702\n",
      "Epoch_Number:  63\n",
      "Training Loss:  0.00177051592618227\n",
      "Epoch_Number:  64\n",
      "Training Loss:  0.0012150666443631053\n",
      "Epoch_Number:  65\n",
      "Training Loss:  0.001350534032098949\n",
      "Epoch_Number:  66\n",
      "Training Loss:  0.0012172290589660406\n",
      "Epoch_Number:  67\n",
      "Training Loss:  0.00115777098108083\n",
      "Epoch_Number:  68\n",
      "Training Loss:  0.0012218760093674064\n",
      "Epoch_Number:  69\n",
      "Training Loss:  0.0009809385519474745\n",
      "Epoch_Number:  70\n",
      "Training Loss:  0.0010839110473170877\n",
      "Epoch_Number:  71\n",
      "Training Loss:  0.0010186488507315516\n",
      "Epoch_Number:  72\n",
      "Training Loss:  0.0010791345266625285\n",
      "Epoch_Number:  73\n",
      "Training Loss:  0.0011576709803193808\n",
      "Epoch_Number:  74\n",
      "Training Loss:  0.0010520241921767592\n",
      "Epoch_Number:  75\n",
      "Training Loss:  0.0010854644933715463\n",
      "Epoch_Number:  76\n",
      "Training Loss:  0.0009140812326222658\n",
      "Epoch_Number:  77\n",
      "Training Loss:  0.0009494720725342631\n",
      "Epoch_Number:  78\n",
      "Training Loss:  0.0008828623685985804\n",
      "Epoch_Number:  79\n",
      "Training Loss:  0.0009128814563155174\n",
      "Epoch_Number:  80\n",
      "Training Loss:  0.0009093799162656069\n",
      "Epoch_Number:  81\n",
      "Training Loss:  0.0008731986745260656\n",
      "Epoch_Number:  82\n",
      "Training Loss:  0.0008908258168958127\n",
      "Epoch_Number:  83\n",
      "Training Loss:  0.0008431943715550005\n",
      "Epoch_Number:  84\n",
      "Training Loss:  0.0008961688145063818\n",
      "Epoch_Number:  85\n",
      "Training Loss:  0.0008692198316566646\n",
      "Epoch_Number:  86\n",
      "Training Loss:  0.0009062087046913803\n",
      "Epoch_Number:  87\n",
      "Training Loss:  0.000864212226588279\n",
      "Epoch_Number:  88\n",
      "Training Loss:  0.0008611399680376053\n",
      "Epoch_Number:  89\n",
      "Training Loss:  0.0008268840028904378\n",
      "Epoch_Number:  90\n",
      "Training Loss:  0.0008244873024523258\n",
      "Epoch_Number:  91\n",
      "Training Loss:  0.000822627276647836\n",
      "Epoch_Number:  92\n",
      "Training Loss:  0.0008176164701581001\n",
      "Epoch_Number:  93\n",
      "Training Loss:  0.0008225252386182547\n",
      "Epoch_Number:  94\n",
      "Training Loss:  0.0008045445429161191\n",
      "Epoch_Number:  95\n",
      "Training Loss:  0.0008122590952552855\n",
      "Epoch_Number:  96\n",
      "Training Loss:  0.0008000053348951042\n",
      "Epoch_Number:  97\n",
      "Training Loss:  0.0008140617865137756\n",
      "Epoch_Number:  98\n",
      "Training Loss:  0.0008052896009758115\n",
      "Epoch_Number:  99\n",
      "Training Loss:  0.0008092864300124347\n",
      "Epoch_Number:  100\n",
      "Training Loss:  0.0007963473326526582\n",
      "Epoch_Number:  101\n",
      "Training Loss:  0.0007938058697618544\n",
      "Epoch_Number:  102\n",
      "Training Loss:  0.0007883745129220188\n",
      "Epoch_Number:  103\n",
      "Training Loss:  0.0007881289930082858\n",
      "Epoch_Number:  104\n",
      "Training Loss:  0.0007880271296016872\n",
      "Epoch_Number:  105\n",
      "Training Loss:  0.0007838799501769245\n",
      "Epoch_Number:  106\n",
      "Training Loss:  0.0007833906565792859\n",
      "Epoch_Number:  107\n",
      "Training Loss:  0.0007782903267070651\n",
      "Epoch_Number:  108\n",
      "Training Loss:  0.0007808335940353572\n",
      "Epoch_Number:  109\n",
      "Training Loss:  0.0007779484149068594\n",
      "Epoch_Number:  110\n",
      "Training Loss:  0.0007801883621141315\n",
      "Epoch_Number:  111\n",
      "Training Loss:  0.0007753735408186913\n",
      "Epoch_Number:  112\n",
      "Training Loss:  0.0007750208023935556\n",
      "Epoch_Number:  113\n",
      "Training Loss:  0.0007706144242547452\n",
      "Epoch_Number:  114\n",
      "Training Loss:  0.0007713076774962246\n",
      "Epoch_Number:  115\n",
      "Training Loss:  0.0007686159224249423\n",
      "Epoch_Number:  116\n",
      "Training Loss:  0.0007687088800594211\n",
      "Epoch_Number:  117\n",
      "Training Loss:  0.0007658587419427931\n",
      "Epoch_Number:  118\n",
      "Training Loss:  0.0007663933211006224\n",
      "Epoch_Number:  119\n",
      "Training Loss:  0.0007636725786142051\n",
      "Epoch_Number:  120\n",
      "Training Loss:  0.0007633241475559771\n",
      "Epoch_Number:  121\n",
      "Training Loss:  0.0007612343179062009\n",
      "Epoch_Number:  122\n",
      "Training Loss:  0.0007611443870700896\n",
      "Epoch_Number:  123\n",
      "Training Loss:  0.0007574773044325411\n",
      "Epoch_Number:  124\n",
      "Training Loss:  0.0007572410395368934\n",
      "Epoch_Number:  125\n",
      "Training Loss:  0.0007545395055785775\n",
      "Epoch_Number:  126\n",
      "Training Loss:  0.000752291816752404\n",
      "Epoch_Number:  127\n",
      "Training Loss:  0.0007494663586840034\n",
      "Epoch_Number:  128\n",
      "Training Loss:  0.0007484300294891\n",
      "Epoch_Number:  129\n",
      "Training Loss:  0.0007463826914317906\n",
      "Epoch_Number:  130\n",
      "Training Loss:  0.0007447861135005951\n",
      "Epoch_Number:  131\n",
      "Training Loss:  0.0007422573980875313\n",
      "Epoch_Number:  132\n",
      "Training Loss:  0.0007417390588670969\n",
      "Epoch_Number:  133\n",
      "Training Loss:  0.0007385030621662736\n",
      "Epoch_Number:  134\n",
      "Training Loss:  0.000737645837944001\n",
      "Epoch_Number:  135\n",
      "Training Loss:  0.0007358859875239432\n",
      "Epoch_Number:  136\n",
      "Training Loss:  0.000733336026314646\n",
      "Epoch_Number:  137\n",
      "Training Loss:  0.0007336452254094183\n",
      "Epoch_Number:  138\n",
      "Training Loss:  0.0007301329751498997\n",
      "Epoch_Number:  139\n",
      "Training Loss:  0.0007298534037545323\n",
      "Epoch_Number:  140\n",
      "Training Loss:  0.0007274459348991513\n",
      "Epoch_Number:  141\n",
      "Training Loss:  0.0007264697924256325\n",
      "Epoch_Number:  142\n",
      "Training Loss:  0.000724941783118993\n",
      "Epoch_Number:  143\n",
      "Training Loss:  0.000723697361536324\n",
      "Epoch_Number:  144\n",
      "Training Loss:  0.0007227938622236252\n",
      "Epoch_Number:  145\n",
      "Training Loss:  0.0007205581059679389\n",
      "Epoch_Number:  146\n",
      "Training Loss:  0.0007200599065981805\n",
      "Epoch_Number:  147\n",
      "Training Loss:  0.0007174931233748794\n",
      "Epoch_Number:  148\n",
      "Training Loss:  0.0007166063296608627\n",
      "Epoch_Number:  149\n",
      "Training Loss:  0.0007148130098357797\n",
      "Epoch_Number:  150\n",
      "Training Loss:  0.0007134625921025872\n",
      "Epoch_Number:  151\n",
      "Training Loss:  0.0007120977970771492\n",
      "Epoch_Number:  152\n",
      "Training Loss:  0.0007105020340532064\n",
      "Epoch_Number:  153\n",
      "Training Loss:  0.0007096984190866351\n",
      "Epoch_Number:  154\n",
      "Training Loss:  0.0007078509661369026\n",
      "Epoch_Number:  155\n",
      "Training Loss:  0.0007074116147123277\n",
      "Epoch_Number:  156\n",
      "Training Loss:  0.0007057162001729012\n",
      "Epoch_Number:  157\n",
      "Training Loss:  0.0007048683473840356\n",
      "Epoch_Number:  158\n",
      "Training Loss:  0.000704111298546195\n",
      "Epoch_Number:  159\n",
      "Training Loss:  0.000702488177921623\n",
      "Epoch_Number:  160\n",
      "Training Loss:  0.0007017215248197317\n",
      "Epoch_Number:  161\n",
      "Training Loss:  0.0007006338564679027\n",
      "Epoch_Number:  162\n",
      "Training Loss:  0.0006996321608312428\n",
      "Epoch_Number:  163\n",
      "Training Loss:  0.0006986993830651045\n",
      "Epoch_Number:  164\n",
      "Training Loss:  0.0006975140422582626\n",
      "Epoch_Number:  165\n",
      "Training Loss:  0.0006966675282455981\n",
      "Epoch_Number:  166\n",
      "Training Loss:  0.0006956937722861767\n",
      "Epoch_Number:  167\n",
      "Training Loss:  0.0006947973743081093\n",
      "Epoch_Number:  168\n",
      "Training Loss:  0.0006939998711459339\n",
      "Epoch_Number:  169\n",
      "Training Loss:  0.0006930779782123864\n",
      "Epoch_Number:  170\n",
      "Training Loss:  0.0006923736655153334\n",
      "Epoch_Number:  171\n",
      "Training Loss:  0.0006914939731359482\n",
      "Epoch_Number:  172\n",
      "Training Loss:  0.0006907973438501358\n",
      "Epoch_Number:  173\n",
      "Training Loss:  0.0006901047308929265\n",
      "Epoch_Number:  174\n",
      "Training Loss:  0.0006894360412843525\n",
      "Epoch_Number:  175\n",
      "Training Loss:  0.0006887757335789502\n",
      "Epoch_Number:  176\n",
      "Training Loss:  0.0006881193839944899\n",
      "Epoch_Number:  177\n",
      "Training Loss:  0.0006875889957882464\n",
      "Epoch_Number:  178\n",
      "Training Loss:  0.0006867545889690518\n",
      "Epoch_Number:  179\n",
      "Training Loss:  0.0006862313020974398\n",
      "Epoch_Number:  180\n",
      "Training Loss:  0.0006853408995084465\n",
      "Epoch_Number:  181\n",
      "Training Loss:  0.0006852701189927757\n",
      "Epoch_Number:  182\n",
      "Training Loss:  0.0006858840934000909\n",
      "Epoch_Number:  183\n",
      "Training Loss:  0.0006852035294286907\n",
      "Epoch_Number:  184\n",
      "Training Loss:  0.0006831975188106298\n",
      "Epoch_Number:  185\n",
      "Training Loss:  0.0006825538002885878\n",
      "Epoch_Number:  186\n",
      "Training Loss:  0.0006832405924797058\n",
      "Epoch_Number:  187\n",
      "Training Loss:  0.0006810259656049311\n",
      "Epoch_Number:  188\n",
      "Training Loss:  0.000680497323628515\n",
      "Epoch_Number:  189\n",
      "Training Loss:  0.0006802019197493792\n",
      "Epoch_Number:  190\n",
      "Training Loss:  0.0006791278719902039\n",
      "Epoch_Number:  191\n",
      "Training Loss:  0.0006779654650017619\n",
      "Epoch_Number:  192\n",
      "Training Loss:  0.0006777029484510422\n",
      "Epoch_Number:  193\n",
      "Training Loss:  0.0006770378677174449\n",
      "Epoch_Number:  194\n",
      "Training Loss:  0.0006758594536222517\n",
      "Epoch_Number:  195\n",
      "Training Loss:  0.0006755976937711239\n",
      "Epoch_Number:  196\n",
      "Training Loss:  0.0006746694562025368\n",
      "Epoch_Number:  197\n",
      "Training Loss:  0.0006743649137206376\n",
      "Epoch_Number:  198\n",
      "Training Loss:  0.0006741583347320557\n",
      "Epoch_Number:  199\n",
      "Training Loss:  0.000674478302244097\n",
      "Epoch_Number:  200\n",
      "Training Loss:  0.0006745834252797067\n",
      "Epoch_Number:  201\n",
      "Training Loss:  0.0006735935457982123\n",
      "Epoch_Number:  202\n",
      "Training Loss:  0.0006723927217535675\n",
      "Epoch_Number:  203\n",
      "Training Loss:  0.0006711368914693594\n",
      "Epoch_Number:  204\n",
      "Training Loss:  0.0006708807777613401\n",
      "Epoch_Number:  205\n",
      "Training Loss:  0.0006708147120662034\n",
      "Epoch_Number:  206\n",
      "Training Loss:  0.0006711671594530344\n",
      "Epoch_Number:  207\n",
      "Training Loss:  0.00067103561013937\n",
      "Epoch_Number:  208\n",
      "Training Loss:  0.0006703914259560406\n",
      "Epoch_Number:  209\n",
      "Training Loss:  0.0006696649943478405\n",
      "Epoch_Number:  210\n",
      "Training Loss:  0.000668809749186039\n",
      "Epoch_Number:  211\n",
      "Training Loss:  0.0006689485162496567\n",
      "Epoch_Number:  212\n",
      "Training Loss:  0.0006692068418487906\n",
      "Epoch_Number:  213\n",
      "Training Loss:  0.0006691504968330264\n",
      "Epoch_Number:  214\n",
      "Training Loss:  0.0006686614360660315\n",
      "Epoch_Number:  215\n",
      "Training Loss:  0.0006678136414848268\n",
      "Epoch_Number:  216\n",
      "Training Loss:  0.0006675437325611711\n",
      "Epoch_Number:  217\n",
      "Training Loss:  0.000667526968754828\n",
      "Epoch_Number:  218\n",
      "Training Loss:  0.0006676421617157757\n",
      "Epoch_Number:  219\n",
      "Training Loss:  0.0006674546166323125\n",
      "Epoch_Number:  220\n",
      "Training Loss:  0.0006670350558124483\n",
      "Epoch_Number:  221\n",
      "Training Loss:  0.0006665325490757823\n",
      "Epoch_Number:  222\n",
      "Training Loss:  0.0006660431972704828\n",
      "Epoch_Number:  223\n",
      "Training Loss:  0.0006658601341769099\n",
      "Epoch_Number:  224\n",
      "Training Loss:  0.0006656995392404497\n",
      "Epoch_Number:  225\n",
      "Training Loss:  0.000665605824906379\n",
      "Epoch_Number:  226\n",
      "Training Loss:  0.0006654207245446742\n",
      "Epoch_Number:  227\n",
      "Training Loss:  0.0006651101284660399\n",
      "Epoch_Number:  228\n",
      "Training Loss:  0.0006647724658250809\n",
      "Epoch_Number:  229\n",
      "Training Loss:  0.000664395047351718\n",
      "Epoch_Number:  230\n",
      "Training Loss:  0.0006640712963417172\n",
      "Epoch_Number:  231\n",
      "Training Loss:  0.0006638631457462907\n",
      "Epoch_Number:  232\n",
      "Training Loss:  0.0006636359030380845\n",
      "Epoch_Number:  233\n",
      "Training Loss:  0.0006634844467043877\n",
      "Epoch_Number:  234\n",
      "Training Loss:  0.0006633353768847883\n",
      "Epoch_Number:  235\n",
      "Training Loss:  0.0006631138967350125\n",
      "Epoch_Number:  236\n",
      "Training Loss:  0.000662921171169728\n",
      "Epoch_Number:  237\n",
      "Training Loss:  0.0006626008544117212\n",
      "Epoch_Number:  238\n",
      "Training Loss:  0.0006623127264901996\n",
      "Epoch_Number:  239\n",
      "Training Loss:  0.000662033271510154\n",
      "Epoch_Number:  240\n",
      "Training Loss:  0.0006617995095439255\n",
      "Epoch_Number:  241\n",
      "Training Loss:  0.0006615708116441965\n",
      "Epoch_Number:  242\n",
      "Training Loss:  0.0006613253499381244\n",
      "Epoch_Number:  243\n",
      "Training Loss:  0.0006610880955122411\n",
      "Epoch_Number:  244\n",
      "Training Loss:  0.0006608552066609263\n",
      "Epoch_Number:  245\n",
      "Training Loss:  0.0006606266251765192\n",
      "Epoch_Number:  246\n",
      "Training Loss:  0.0006604362279176712\n",
      "Epoch_Number:  247\n",
      "Training Loss:  0.0006601635250262916\n",
      "Epoch_Number:  248\n",
      "Training Loss:  0.0006600141059607267\n",
      "Epoch_Number:  249\n",
      "Training Loss:  0.0006600454798899591\n",
      "Epoch_Number:  250\n",
      "Training Loss:  0.0006602279609069228\n",
      "Epoch_Number:  251\n",
      "Training Loss:  0.0006606547976844013\n",
      "Epoch_Number:  252\n",
      "Training Loss:  0.0006612929864786565\n",
      "Epoch_Number:  253\n",
      "Training Loss:  0.0006622510845772922\n",
      "Epoch_Number:  254\n",
      "Training Loss:  0.0006625092937611043\n",
      "Epoch_Number:  255\n",
      "Training Loss:  0.0006621842039749026\n",
      "Epoch_Number:  256\n",
      "Training Loss:  0.0006606578244827688\n",
      "Epoch_Number:  257\n",
      "Training Loss:  0.000659063458442688\n",
      "Epoch_Number:  258\n",
      "Training Loss:  0.0006584839429706335\n",
      "Epoch_Number:  259\n",
      "Training Loss:  0.0006590781849808991\n",
      "Epoch_Number:  260\n",
      "Training Loss:  0.0006600816268473864\n",
      "Epoch_Number:  261\n",
      "Training Loss:  0.000660361722111702\n",
      "Epoch_Number:  262\n",
      "Training Loss:  0.0006597800529561937\n",
      "Epoch_Number:  263\n",
      "Training Loss:  0.0006583196227438748\n",
      "Epoch_Number:  264\n",
      "Training Loss:  0.0006573431310243905\n",
      "Epoch_Number:  265\n",
      "Training Loss:  0.0006572736310772598\n",
      "Epoch_Number:  266\n",
      "Training Loss:  0.0006577254389412701\n",
      "Epoch_Number:  267\n",
      "Training Loss:  0.0006579284090548754\n",
      "Epoch_Number:  268\n",
      "Training Loss:  0.0006576087325811386\n",
      "Epoch_Number:  269\n",
      "Training Loss:  0.000656999705825001\n",
      "Epoch_Number:  270\n",
      "Training Loss:  0.0006564927753061056\n",
      "Epoch_Number:  271\n",
      "Training Loss:  0.000656257732771337\n",
      "Epoch_Number:  272\n",
      "Training Loss:  0.0006562910275533795\n",
      "Epoch_Number:  273\n",
      "Training Loss:  0.0006566813681274652\n",
      "Epoch_Number:  274\n",
      "Training Loss:  0.0006569979595951736\n",
      "Epoch_Number:  275\n",
      "Training Loss:  0.0006569548859260976\n",
      "Epoch_Number:  276\n",
      "Training Loss:  0.0006565399235114455\n",
      "Epoch_Number:  277\n",
      "Training Loss:  0.0006558314198628068\n",
      "Epoch_Number:  278\n",
      "Training Loss:  0.0006553218117915094\n",
      "Epoch_Number:  279\n",
      "Training Loss:  0.0006550385151058435\n",
      "Epoch_Number:  280\n",
      "Training Loss:  0.0006550807156600058\n",
      "Epoch_Number:  281\n",
      "Training Loss:  0.0006553252460435033\n",
      "Epoch_Number:  282\n",
      "Training Loss:  0.0006555146537721157\n",
      "Epoch_Number:  283\n",
      "Training Loss:  0.0006555604632012546\n",
      "Epoch_Number:  284\n",
      "Training Loss:  0.000655434443615377\n",
      "Epoch_Number:  285\n",
      "Training Loss:  0.0006552047561854124\n",
      "Epoch_Number:  286\n",
      "Training Loss:  0.0006548440433107316\n",
      "Epoch_Number:  287\n",
      "Training Loss:  0.000654427392873913\n",
      "Epoch_Number:  288\n",
      "Training Loss:  0.0006539421156048775\n",
      "Epoch_Number:  289\n",
      "Training Loss:  0.0006535965949296951\n",
      "Epoch_Number:  290\n",
      "Training Loss:  0.0006534019485116005\n",
      "Epoch_Number:  291\n",
      "Training Loss:  0.0006532786646857858\n",
      "Epoch_Number:  292\n",
      "Training Loss:  0.0006533758132718503\n",
      "Epoch_Number:  293\n",
      "Training Loss:  0.0006538212765008211\n",
      "Epoch_Number:  294\n",
      "Training Loss:  0.0006544558564200997\n",
      "Epoch_Number:  295\n",
      "Training Loss:  0.0006557717570103705\n",
      "Epoch_Number:  296\n",
      "Training Loss:  0.0006569932447746396\n",
      "Epoch_Number:  297\n",
      "Training Loss:  0.0006574647850356996\n",
      "Epoch_Number:  298\n",
      "Training Loss:  0.0006572289858013391\n",
      "Epoch_Number:  299\n",
      "Training Loss:  0.0006557729211635888\n"
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    cnn.train()\n",
    "    lstm.train()\n",
    "    cnn.to(device)\n",
    "    lstm.to(device)\n",
    "    print('Epoch_Number: ', epoch)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()      \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        feature = cnn(data)\n",
    "        #print(feature.shape)\n",
    "        feature = feature.reshape((feature.shape[0],1,feature.shape[1]))\n",
    "        #print(feature.shape)\n",
    "        outputs = lstm(feature)\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        loss = criterion(outputs, target.reshape(-1,1))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    running_loss /= len(train_loader)\n",
    "    training_loss.append(running_loss)\n",
    "    print('Training Loss: ', running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in lstm.parameters():\n",
    "#     print(param.data) \n",
    "#     print(len(param.data))\n",
    "#     #x = param.data[0].clone()\n",
    "#     x = param.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(param.data.shape)\n",
    "# torch.equal(x,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "# 8: forecast\n",
    "# 18: previous\n",
    "# 19: evalidate\n",
    "for predict_day in range(4):\n",
    "    out = []\n",
    "\n",
    "    lstm.eval()\n",
    "    lstm.to(device)\n",
    "\n",
    "    for data in forecast_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "\n",
    "        #outputs = lstm(data)\n",
    "        feature = cnn(data)\n",
    "        feature = feature.reshape((feature.shape[0],1,feature.shape[1]))\n",
    "        #print(feature.shape)\n",
    "        outputs = lstm(feature)\n",
    "        out.append(outputs)\n",
    "\n",
    "\n",
    "    out_cat = torch.cat(out)\n",
    "    out_cp = out_cat.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    policy = forecasting[:, 1:-1]\n",
    "    pre_X = np.concatenate((out_cp,policy), axis = 1)\n",
    "    pre_X = pre_X.reshape((pre_X.shape[0], 1, pre_X.shape[1]))\n",
    "    dataset_forecast = forecast(pre_X)\n",
    "    forecast_loader = DataLoader(dataset_forecast,shuffle=False,batch_size=1)\n",
    "    predict.append(out_cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "print(forecast_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.24017373]], dtype=float32),\n",
       " array([[0.2809428]], dtype=float32),\n",
       " array([[0.29090858]], dtype=float32),\n",
       " array([[0.2933832]], dtype=float32)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "forecast_X = forecast_X.reshape((forecast_X.shape[0], forecast_X.shape[2]))\n",
    "for i in range(len(predict)):      \n",
    "    inv_yhat = concatenate((predict[i], forecast_X[:, 1:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    outputs.append(inv_yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([18892.855], dtype=float32),\n",
       " array([22099.885], dtype=float32),\n",
       " array([22883.824], dtype=float32),\n",
       " array([23078.488], dtype=float32)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_forecasting.data.examples import get_stallion_data\n",
    "data = pd.read_csv(\"/home/mail12/Nerf-diffusion/covid_19/FINAL/TRANSFORMER/forecast_brazil.csv\")\n",
    "#data\n",
    "data = data.drop(columns=['Unnamed: 0','date'])\n",
    "cols = data.columns.tolist()\n",
    "cols = cols[-1:]+cols[:-1]\n",
    "cols = ['new_cases_smoothed', 'stringency_index', 'face_coverings', 'Gathering',\n",
    "       'Transport', 'Stay_home']\n",
    "k3 =data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 12354.723\n",
      "Test MAE: 12307.370\n"
     ]
    }
   ],
   "source": [
    "# validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "# d = pd.read_csv(\"/Users/xul2/Desktop/CMU/Research/Covid 19/7-Day Average.csv\")\n",
    "# d_interest = d.loc[d[\"Country\"] == \"Russia\"]\n",
    "# y = d_interest[\"7-day-new\"].values[194:211]\n",
    "# drop the first prediction, which is based on the true case\n",
    "y = k3['new_cases_smoothed'].values[11:15]\n",
    "y_pre = outputs\n",
    "rmse = sqrt(mean_squared_error(y, y_pre))\n",
    "mae = mean_absolute_error(y,y_pre)\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/mail12/Nerf-diffusion/covid_19/FINAL/LSTM_CNN/'\n",
    "np.save(folder_path + 'brazil_metrics.npy', np.array([mae, rmse]))\n",
    "np.save(folder_path + 'brazil_results.npy', np.array([y, y_pre]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative case\n",
    "# d = pd.read_csv(\"/Users/xul2/Desktop/CMU/Research/Covid 19/7-Day Average.csv\")\n",
    "# d_interest = d.loc[d[\"Country\"] == \"Russia\"]\n",
    "# y = d_interest[\"Cumulative\"].values[194:212]\n",
    "# # y = d_interest[\"Confirmed\"].values[131:148]\n",
    "# print(y)\n",
    "# y_pre = []\n",
    "# # Brazil\n",
    "# b = 19151993\n",
    "# # India\n",
    "# b = 30946147\n",
    "# # Russia\n",
    "# b = 5762211\n",
    "# for i in range(len(outputs)-1):\n",
    "#     b = b+outputs[i+1]\n",
    "#     y_pre.append(b)\n",
    "# df = pd.DataFrame(y_pre)\n",
    "# df.to_csv('hybrid_russia_pre.csv',index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous study\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.metrics import r2_score\n",
    "# from sklearn.metrics import explained_variance_score\n",
    "# d = pd.read_csv(\"/Users/xul2/Desktop/CMU/Research/Covid 19/previous.csv\")\n",
    "# d_interest = d.loc[d[\"Country\"] == \"France\"]\n",
    "# y = d_interest[\"Confirmed\"].values[131:148]\n",
    "# y_pre = []\n",
    "# # Italy\n",
    "# #b = 232997\n",
    "# # # Spain\n",
    "# b = 239479\n",
    "# # # France\n",
    "# b = 190975\n",
    "# # # US\n",
    "# #b = 1798718\n",
    "# for i in range(len(outputs)-1):\n",
    "#     b = b+outputs[i+1]\n",
    "#     y_pre.append(b)\n",
    "# rmse = sqrt(mean_squared_error(y, y_pre))\n",
    "# mae = mean_absolute_error(y,y_pre)\n",
    "# print('Test RMSE: %.3f' % rmse)\n",
    "# print('Test MAE: %.3f' % mae)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "kernelspec": {
   "display_name": "vision_heat",
   "language": "python",
   "name": "vision_heat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
